{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "local_config_file = \"./local.settings.json\"\n",
    "\n",
    "with open(local_config_file) as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "api_path = config[\"DATA\"][\"API\"][\"Path\"]\n",
    "api_key = config[\"DATA\"][\"API\"][\"Key\"]\n",
    "user_id = config[\"DATA\"][\"API\"][\"UserID\"]\n",
    "organization_id = config[\"DATA\"][\"API\"][\"OrganizationID\"]\n",
    "user_name = config[\"DATA\"][\"API\"][\"UserName\"]\n",
    "process_files = config[\"DATA\"][\"API\"][\"Methods\"][\"ProcessFiles\"]\n",
    "query_files = config[\"DATA\"][\"API\"][\"Methods\"][\"QueryFiles\"]\n",
    "\n",
    "local_files_folder = config[\"Local\"][\"FilesFolder\"]\n",
    "\n",
    "output_csv_file_path = \"./output/\"\n",
    "output_csv_file_name = None\n",
    "\n",
    "#print(local_files_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "files_to_upload = []\n",
    "\n",
    "for root, dirs, files in os.walk(local_files_folder):\n",
    "    for file in files:\n",
    "        if file.endswith(\".txt\"):\n",
    "            files_to_upload.append(os.path.join(root, file))\n",
    "            #print(os.path.join(root, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "    \n",
    "def get_mime_type(file_path:str):\n",
    "    mime_type = \"application/octet-stream\"\n",
    "\n",
    "    if file_path.endswith(\".pdf\"):\n",
    "        mime_type = \"application/pdf\"\n",
    "    elif file_path.endswith(\".docx\"):\n",
    "        mime_type = \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\"\n",
    "    elif file_path.endswith(\".doc\"):\n",
    "        mime_type = \"application/msword\"\n",
    "    elif file_path.endswith(\".txt\"):\n",
    "        mime_type = \"text/plain\"\n",
    "\n",
    "    return mime_type\n",
    "\n",
    "class FileProcessingInputData:\n",
    "    UserId:str\n",
    "    OrganizationId:str\n",
    "    UserName:str\n",
    "    FileId:str\n",
    "    ScoreMarginalDocuments:bool\n",
    "    Tags:List[str]\n",
    "    Properties:dict\n",
    "\n",
    "def get_file_data(file_path:str):\n",
    "    file_data = FileProcessingInputData()\n",
    "    file_data.UserId = user_id\n",
    "    file_data.OrganizationId = organization_id\n",
    "    file_data.UserName = user_name\n",
    "    file_data.FileId = os.path.basename(file_path)\n",
    "    file_data.ScoreMarginalDocuments = True\n",
    "\n",
    "    return file_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process started at 2023-04-19T15:43:25\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from io import StringIO\n",
    "from datetime import datetime\n",
    "\n",
    "api_process_files_path = f'{api_path}/{process_files}'\n",
    "\n",
    "process_start = datetime.now()\n",
    "process_start_str = process_start.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "\n",
    "print(f\"Process started at {process_start_str}\")\n",
    "\n",
    "def upload_document(file_data:FileProcessingInputData, file_path:str):\n",
    "    \n",
    "    url = api_process_files_path\n",
    "    headers = {\"Data-Subscription-Key\": api_key}\n",
    "\n",
    "    data_json = StringIO(json.dumps(file_data, default=vars))\n",
    "    file = open(file_path, \"rb\").read()\n",
    "    files = {\"json\": (None, data_json, \"application/json; charset=UTF-8\"), file_data.FileId: (file_data.FileId, file, get_mime_type(file_path))}\n",
    "\n",
    "    response = requests.post(url, headers=headers, files=files)\n",
    "    print(str(response.status_code) + \" \" + response.reason + \" \" + response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202 Accepted PROCESSING\n",
      "202 Accepted PROCESSING\n",
      "202 Accepted PROCESSING\n",
      "202 Accepted PROCESSING\n",
      "202 Accepted PROCESSING\n",
      "202 Accepted PROCESSING\n"
     ]
    }
   ],
   "source": [
    "for file_path in files_to_upload:\n",
    "    file_data = get_file_data(file_path)\n",
    "    upload_document(file_data, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from python_graphql_client import GraphqlClient\n",
    "\n",
    "api_query_files_path = f'{api_path}/{query_files}'\n",
    "\n",
    "# Your GraphQL client needs the api path and the api key\n",
    "graphql_client = GraphqlClient(endpoint=api_query_files_path, headers={\"Data-Subscription-Key\":api_key})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query{mydata(userId: \"4f540842-9d85-4bbd-8271-aa1505b888f7\", orderBy: {path: \"DateUploaded\", descending: true},where:[{groupedExpressions: [{path: \"hasMyDocumentDeleted\", comparison: equal, value: \"false\"},{ path: \"dateUploaded\", comparison: greaterThanOrEqual, value: \"2023-04-19T15:43:25\" }]}],first:100){totalCount,edges{node{id,fullPath,hasMyDocumentScored,error,documentExtension,dateUploaded,name,scorablePageCount,documentSize,myDocumentScore{dataScore,deceptiveFragmentsCount,deceptiveFragmentsPercentage}}},pageInfo{startCursor,endCursor,hasPreviousPage,hasNextPage}}}\n"
     ]
    }
   ],
   "source": [
    "graphql_query = \"query{mydata(userId: \\\"\" + user_id  + \"\\\", orderBy: {path: \\\"DateUploaded\\\", descending: true},where:[{groupedExpressions: [{path: \\\"hasMyDocumentDeleted\\\", comparison: equal, value: \\\"false\\\"},{ path: \\\"dateUploaded\\\", comparison: greaterThanOrEqual, value: \\\"\" + process_start_str + \"\\\" }]}],first:100){totalCount,edges{node{id,fullPath,hasMyDocumentScored,error,documentExtension,dateUploaded,name,scorablePageCount,documentSize,myDocumentScore{dataScore,deceptiveFragmentsCount,deceptiveFragmentsPercentage}}},pageInfo{startCursor,endCursor,hasPreviousPage,hasNextPage}}}\"\n",
    "\n",
    "print (graphql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ContentTypeError",
     "evalue": "0, message='Attempt to decode JSON with unexpected mimetype: ', url=URL('https://d-a-t-a.azure-api.net/my-data-files/v1.0')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mContentTypeError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m graphql_result \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m graphql_client\u001b[39m.\u001b[39mexecute_async(query\u001b[39m=\u001b[39mgraphql_query)\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mok\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Code\\DT\\Github\\api-notebooks\\.venv\\lib\\site-packages\\python_graphql_client\\graphql_client.py:75\u001b[0m, in \u001b[0;36mGraphqlClient.execute_async\u001b[1;34m(self, query, variables, operation_name, headers)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mwith\u001b[39;00m aiohttp\u001b[39m.\u001b[39mClientSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m     70\u001b[0m     \u001b[39masync\u001b[39;00m \u001b[39mwith\u001b[39;00m session\u001b[39m.\u001b[39mpost(\n\u001b[0;32m     71\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mendpoint,\n\u001b[0;32m     72\u001b[0m         json\u001b[39m=\u001b[39mrequest_body,\n\u001b[0;32m     73\u001b[0m         headers\u001b[39m=\u001b[39m{\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheaders, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mheaders},\n\u001b[0;32m     74\u001b[0m     ) \u001b[39mas\u001b[39;00m response:\n\u001b[1;32m---> 75\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m response\u001b[39m.\u001b[39mjson()\n",
      "File \u001b[1;32md:\\Code\\DT\\Github\\api-notebooks\\.venv\\lib\\site-packages\\aiohttp\\client_reqrep.py:1104\u001b[0m, in \u001b[0;36mClientResponse.json\u001b[1;34m(self, encoding, loads, content_type)\u001b[0m\n\u001b[0;32m   1102\u001b[0m     ctype \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(hdrs\u001b[39m.\u001b[39mCONTENT_TYPE, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mlower()\n\u001b[0;32m   1103\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_expected_content_type(ctype, content_type):\n\u001b[1;32m-> 1104\u001b[0m         \u001b[39mraise\u001b[39;00m ContentTypeError(\n\u001b[0;32m   1105\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_info,\n\u001b[0;32m   1106\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistory,\n\u001b[0;32m   1107\u001b[0m             message\u001b[39m=\u001b[39m(\n\u001b[0;32m   1108\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mAttempt to decode JSON with \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39munexpected mimetype: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m ctype\n\u001b[0;32m   1109\u001b[0m             ),\n\u001b[0;32m   1110\u001b[0m             headers\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheaders,\n\u001b[0;32m   1111\u001b[0m         )\n\u001b[0;32m   1113\u001b[0m stripped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_body\u001b[39m.\u001b[39mstrip()  \u001b[39m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m   1114\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m stripped:\n",
      "\u001b[1;31mContentTypeError\u001b[0m: 0, message='Attempt to decode JSON with unexpected mimetype: ', url=URL('https://d-a-t-a.azure-api.net/my-data-files/v1.0')"
     ]
    }
   ],
   "source": [
    "graphql_result = await graphql_client.execute_async(query=graphql_query)\n",
    "\n",
    "print('ok')\n",
    "\n",
    "try:\n",
    "    print(graphql_result[\"data\"][\"mydata\"][\"totalCount\"])\n",
    "except Exception as ex:\n",
    "    print(graphql_result)\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "flattened_results = []\n",
    "\n",
    "for edge in graphql_result[\"data\"][\"mydata\"][\"edges\"]:\n",
    "    flattened:dict = dict()\n",
    "    flattened[\"ID\"] = edge[\"node\"][\"id\"]\n",
    "    flattened[\"Name\"] = edge[\"node\"][\"name\"]\n",
    "    flattened[\"Full Path\"] = edge[\"node\"][\"fullPath\"]\n",
    "    flattened[\"Scored\"] = edge[\"node\"][\"hasMyDocumentScored\"]\n",
    "    flattened[\"Error\"] = edge[\"node\"][\"error\"]\n",
    "    flattened[\"Extension\"] = edge[\"node\"][\"documentExtension\"]\n",
    "    flattened[\"Date Uploaded\"] = edge[\"node\"][\"dateUploaded\"]\n",
    "    flattened[\"Fiscal Quarter\"] = edge[\"node\"][\"companyDataScore\"]\n",
    "    flattened[\"Scorable Page Count\"] = edge[\"node\"][\"scorablePageCount\"]\n",
    "    flattened[\"Document Bytes\"] = edge[\"node\"][\"documentSize\"]\n",
    "    flattened[\"DATA Score\"] = edge[\"node\"][\"myDocumentScore\"][\"dataScore\"]\n",
    "    flattened[\"Deceptive Fragments Count\"] = edge[\"node\"][\"myDocumentScore\"][\"deceptiveFragmentsCount\"]\n",
    "    flattened[\"Deceptive Fragments Percentage\"] = edge[\"node\"][\"myDocumentScore\"][\"deceptiveFragmentsPercentage\"]\n",
    "    flattened_results.append(flattened)\n",
    "\n",
    "df = pd.read_json(json.dumps(flattened_results))\n",
    "\n",
    "file_path = (output_csv_file_path + output_csv_file_name) if not output_csv_file_name is None else output_csv_file_path + \"output-\" + str(datetime.now().timestamp()) + \".csv\"\n",
    "\n",
    "df.to_csv(file_path, encoding='utf-8', index=False)\n",
    "\n",
    "print(\"Wrote \" + str(len(flattened_results)) + \" to csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
